 name: CI Pipeline for Azure Databricks

 on:
   push:
     branches:
       - master
   pull_request:
     branches:
       - master

 jobs:
   deploy:
     runs-on: ubuntu-latest

     steps:
     - name: Checkout code
       uses: actions/checkout@v3

     - name: Set up Python
       uses: actions/setup-python@v4
       with:
         python-version: '3.x'

     - name: Install Databricks CLI
       run: |
         pip install databricks-cli

     - name: Configure Databricks CLI
       run: |
         databricks configure --token <<EOF
         ${{ secrets.DATABRICKS_HOST }}
         ${{ secrets.DATABRICKS_TOKEN }}
         EOF

     - name: Upload sample data to DBFS
       run: databricks fs cp sample_sales.csv dbfs:/FileStore/sample_sales.csv --overwrite

     - name: Import Notebook to Workspace
       run: databricks workspace import sample_sales_notebook.py "/Workspace/Users/n.krzysiek@gmail.com/Untitled Notebook 2025-04-12 14:37:52" -l python --overwrite

       env:
         DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

     - name: Run Databricks Job
       run: |
         databricks jobs create --json-file job-config.json
         databricks jobs run-now --job-id $(databricks jobs list | grep -m 1 'CD pipeline' | awk '{print $1}')
       env:
         DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
